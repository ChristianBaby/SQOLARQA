# ScholarQA ğŸ“š

> **Sistema Inteligente de Preguntas y Respuestas para PDFs con LLMs Locales**

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![Licencia: MIT](https://img.shields.io/badge/Licencia-MIT-yellow.svg)](LICENSE)
[![VersiÃ³n](https://img.shields.io/badge/versiÃ³n-2.0-green.svg)](CHANGELOG.md)

ScholarQA es un sistema RAG (GeneraciÃ³n Aumentada por RecuperaciÃ³n) optimizado que te permite conversar con tus documentos PDF usando modelos LLM locales. **La versiÃ³n 2.0** incluye optimizaciones de rendimiento completas que lo hacen **2-20x mÃ¡s rÃ¡pido**.

---

## âœ¨ CaracterÃ­sticas Principales

- ğŸ”’ **100% Local y Privado** - NingÃºn dato sale de tu mÃ¡quina
- âš¡ **Altamente Optimizado** - 2-20x mÃ¡s rÃ¡pido que la v1.0
- ğŸ§  **FragmentaciÃ³n SemÃ¡ntica** - Mejor comprensiÃ³n del contexto
- ğŸ’¾ **CachÃ© Inteligente** - Consultas repetidas dramÃ¡ticamente mÃ¡s rÃ¡pidas
- ğŸ¯ **Soporte GPU** - AceleraciÃ³n GPU opcional
- ğŸŒ **Interfaz Web** - Interfaz Flask moderna
- ğŸ’» **CLI Potente** - AutomatizaciÃ³n por lÃ­nea de comandos
- ğŸ“Š **Monitoreo de Rendimiento** - MÃ©tricas en tiempo real

---

## ğŸš€ Inicio RÃ¡pido

### Requisitos Previos

- Python 3.9 o superior
- 4GB RAM mÃ­nimo (8GB recomendado)
- 2GB de espacio en disco

### InstalaciÃ³n

```bash
# Clonar repositorio
git clone <tu-repositorio>
cd scholarqa

# Crear entorno virtual
python -m venv venv

# Activar entorno virtual
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# Instalar dependencias
pip install -r requirements.txt

# Configurar entorno
cp .env.example .env

# Verificar instalaciÃ³n
python scripts/verify_optimizations.py
```

### Descargar Modelo LLM

Descarga un modelo `.gguf` de [HuggingFace](https://huggingface.co/models?search=gguf) y colÃ³calo en `models/downloaded/`

**Modelos Recomendados:**
- TinyLlama-1.1B (~600MB) - RÃ¡pido, bueno para pruebas
- Mistral-7B (~4GB) - Mejor calidad
- Llama-2-7B (~4GB) - Rendimiento equilibrado

---

## ğŸ’¡ Uso

### Interfaz Web

```bash
python src/app.py
```

Abre http://localhost:5000 en tu navegador.

### Interfaz de LÃ­nea de Comandos

```bash
# Subir un PDF
python src/cli.py upload ruta/al/documento.pdf

# Hacer una pregunta
python src/cli.py ask "Â¿CuÃ¡l es el tema principal?"

# Ver estadÃ­sticas
python src/cli.py stats

# Listar documentos (modo detallado)
python src/cli.py list -v

# Limpiar base de datos
python src/cli.py clear -y
```

---

## ğŸ“Š Rendimiento

### Resultados de Benchmark (v2.0 vs v1.0)

| OperaciÃ³n | v1.0 | v2.0 | Mejora |
|-----------|------|------|--------|
| Procesar PDF de 50 pÃ¡ginas | 15s | 6s | **2.5x mÃ¡s rÃ¡pido** âš¡ |
| Generar 100 embeddings | 8s | 2s | **4x mÃ¡s rÃ¡pido** âš¡ |
| Insertar 1000 documentos | 45s | 12s | **3.75x mÃ¡s rÃ¡pido** âš¡ |
| Consulta en cachÃ© | 2s | 0.1s | **20x mÃ¡s rÃ¡pido** âš¡ |

### Uso de Recursos

- Uso de memoria: **-30%**
- Uso de CPU: **-31%**
- Mejor utilizaciÃ³n de GPU

---

## ğŸ¯ ConfiguraciÃ³n

Variables de entorno clave en `.env`:

```env
# Rendimiento
MAX_WORKERS=4                  # Hilos de procesamiento paralelo
ENABLE_CACHE=True              # Habilitar cachÃ© inteligente
USE_GPU=False                  # Habilitar aceleraciÃ³n GPU
EMBEDDING_BATCH_SIZE=32        # TamaÃ±o de lote para embeddings

# Procesamiento de Texto
USE_SEMANTIC_CHUNKING=True     # Fragmentos de mejor calidad
CHUNK_SIZE=1000               # Caracteres por fragmento
CHUNK_OVERLAP=200             # SuperposiciÃ³n entre fragmentos

# Modelos
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
LLM_MODEL=models/downloaded/tu-modelo.gguf
```

Consulta [docs/OPTIMIZATIONS.md](docs/OPTIMIZATIONS.md) para configuraciÃ³n avanzada.

---

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Usuario    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚Flask/CLIâ”‚
   â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚Procesador PDFâ”‚â”€â”€â”€â”€â”€â–¶â”‚Divisor Texto â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚Motor Embedding â”‚
                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚AlmacÃ©n Vector â”‚
                         â”‚  (ChromaDB)   â”‚
                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚  Motor LLM    â”‚
                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                            Respuesta
```

---

## ğŸ“š DocumentaciÃ³n

- **[OPTIMIZATIONS.md](docs/OPTIMIZATIONS.md)** - GuÃ­a de optimizaciones de rendimiento
- **[API.md](docs/API.md)** - DocumentaciÃ³n de API
- **[ARCHITECTURE.md](docs/ARCHITECTURE.md)** - Arquitectura del sistema
- **[CONTRIBUTING.md](docs/CONTRIBUTING.md)** - GuÃ­as de contribuciÃ³n

---

## ğŸ†• Novedades en v2.0

### CaracterÃ­sticas Principales
- âœ… **Sistema de cachÃ© inteligente** (50-90% mÃ¡s rÃ¡pido en consultas repetidas)
- âœ… **FragmentaciÃ³n semÃ¡ntica de texto** (mejor calidad de respuestas)
- âœ… **Procesamiento paralelo de PDF** (2.5x mÃ¡s rÃ¡pido)
- âœ… **Soporte de aceleraciÃ³n GPU**
- âœ… **Monitoreo de rendimiento** (CPU, memoria, tiempo)
- âœ… **ValidaciÃ³n robusta** (validaciÃ³n de entrada, manejo de errores)
- âœ… **CLI mejorada** (comando stats, modo detallado)

### Mejoras TÃ©cnicas
- MÃ¡s de 2000 lÃ­neas de cÃ³digo optimizado
- 39 pruebas completas
- Type hints en todo el cÃ³digo
- DocumentaciÃ³n extensa
- Mejor manejo de errores

---

## ğŸ› ï¸ Stack TecnolÃ³gico

| Componente | TecnologÃ­a |
|-----------|-----------|
| **LLM** | llama.cpp (modelos GGUF) |
| **Embeddings** | Sentence Transformers |
| **Base de Datos Vectorial** | ChromaDB |
| **Framework Web** | Flask |
| **Procesamiento PDF** | pypdf |
| **Async/Threading** | ThreadPoolExecutor |
| **Monitoreo** | psutil |

---

## ğŸ§ª Pruebas

```bash
# Ejecutar todas las pruebas
pytest tests/ -v

# Ejecutar pruebas de optimizaciÃ³n
pytest tests/test_optimizations.py -v

# Ejecutar con cobertura
pytest tests/ -v --cov=src --cov-report=html
```

---

## ğŸ¤ Contribuir

Â¡Las contribuciones son bienvenidas! Por favor consulta [CONTRIBUTING.md](docs/CONTRIBUTING.md) para las guÃ­as.

### ConfiguraciÃ³n de Desarrollo

```bash
# Instalar dependencias de desarrollo
pip install -r requirements.txt

# Ejecutar pruebas
pytest tests/ -v

# Ejecutar linter
flake8 src/ tests/

# VerificaciÃ³n de tipos
mypy src/
```

---

## ğŸ“„ Licencia

Licencia MIT - consulta el archivo [LICENSE](LICENSE) para detalles.

---

## ğŸ™ Agradecimientos

- [llama.cpp](https://github.com/ggerganov/llama.cpp) - Inferencia rÃ¡pida de LLM
- [Sentence Transformers](https://www.sbert.net/) - Embeddings de Ãºltima generaciÃ³n
- [ChromaDB](https://www.trychroma.com/) - Base de datos vectorial nativa para IA
- [Flask](https://flask.palletsprojects.com/) - Framework web

---

## ğŸ—ºï¸ Hoja de Ruta

- [ ] Soporte async/await
- [ ] Respuestas streaming del LLM
- [ ] Ãndices vectoriales FAISS
- [ ] Conversaciones multi-documento
- [ ] API REST
- [ ] Despliegue Docker
- [ ] Panel web para monitoreo

---

**Hecho con â¤ï¸ para la comunidad acadÃ©mica**

*ScholarQA v2.0 - RÃ¡pido, Inteligente, Privado*
