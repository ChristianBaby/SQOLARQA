# Comenzando con ScholarQA

Esta gu√≠a te ayudar√° a tener ScholarQA funcionando en minutos.

---

## Requisitos Previos

Antes de comenzar, aseg√∫rate de tener:

- ‚úÖ **Python 3.9 o superior** instalado
- ‚úÖ **4GB RAM** m√≠nimo (8GB recomendado)
- ‚úÖ **2GB de espacio en disco** para modelos y datos
- ‚úÖ **Git** (para clonar el repositorio)

---

## Paso 1: Instalaci√≥n

### Clonar el Repositorio

```bash
git clone <url-de-tu-repositorio>
cd scholarqa
```

### Crear Entorno Virtual

**Windows:**
```bash
python -m venv venv
venv\Scripts\activate
```

**Linux/Mac:**
```bash
python3 -m venv venv
source venv/bin/activate
```

### Instalar Dependencias

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

---

## Paso 2: Configuraci√≥n

### Copiar Archivo de Entorno

```bash
cp .env.example .env
```

### Configuraci√≥n B√°sica

Edita `.env` con tus configuraciones preferidas:

```env
# Configuraciones esenciales
ENABLE_CACHE=True
USE_SEMANTIC_CHUNKING=True
MAX_WORKERS=4

# Rutas de modelos (ajustar si es necesario)
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
LLM_MODEL=models/downloaded/tu-modelo.gguf
```

---

## Paso 3: Descargar Modelo LLM

Descarga un modelo GGUF de HuggingFace:

### Modelos Recomendados

**Para Pruebas (R√°pido):**
- [TinyLlama-1.1B-Chat](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF) (~600MB)

**Para Producci√≥n (Calidad):**
- [Mistral-7B-Instruct](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF) (~4GB)
- [Llama-2-7B-Chat](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF) (~4GB)

### Ejemplo de Descarga

```bash
# Crear directorio de modelos
mkdir -p models/downloaded

# Descargar con wget (Linux/Mac) o usar el navegador
wget https://huggingface.co/.../model.gguf -O models/downloaded/model.gguf
```

Actualiza `.env` con la ruta del modelo:
```env
LLM_MODEL=models/downloaded/model.gguf
```

---

## Paso 4: Verificar Instalaci√≥n

Ejecuta el script de verificaci√≥n:

```bash
python scripts/verify_optimizations.py
```

Esto verificar√°:
- ‚úÖ Todas las dependencias instaladas
- ‚úÖ Estructura de directorios
- ‚úÖ Configuraci√≥n v√°lida
- ‚úÖ Modelos disponibles
- ‚úÖ Rendimiento del sistema

---

## Paso 5: Primera Ejecuci√≥n

### Interfaz Web

Iniciar el servidor web:

```bash
python src/app.py
```

Abre tu navegador en: **http://localhost:5000**

### L√≠nea de Comandos

Prueba estos comandos:

```bash
# Ver ayuda
python src/cli.py --help

# Ver estad√≠sticas del sistema
python src/cli.py stats

# Subir un PDF (usa una muestra de data/pdfs/)
python src/cli.py upload data/pdfs/sample.pdf

# Hacer una pregunta
python src/cli.py ask "¬øCu√°l es el tema principal del documento?"
```

---

## Paso 6: Subir Tu Primer Documento

### V√≠a Interfaz Web

1. Abre http://localhost:5000
2. Haz clic en "Subir PDF"
3. Selecciona tu archivo PDF
4. Espera el procesamiento
5. Haz preguntas en el chat

### V√≠a CLI

```bash
# Subir
python src/cli.py upload ruta/a/tu/documento.pdf

# Preguntar
python src/cli.py ask "Resume la metodolog√≠a"
```

---

## Problemas Comunes y Soluciones

### Problema: "Modelo no encontrado"

**Soluci√≥n:**
1. Descarga un modelo GGUF (ver Paso 3)
2. Actualiza `LLM_MODEL` en `.env`
3. Verifica con: `python scripts/verify_optimizations.py`

### Problema: "Sin memoria"

**Soluci√≥n:**
Reduce el uso de recursos en `.env`:
```env
MAX_WORKERS=2
EMBEDDING_BATCH_SIZE=16
CHUNK_SIZE=800
```

### Problema: "Rendimiento lento"

**Soluci√≥n:**
Habilita optimizaciones en `.env`:
```env
ENABLE_CACHE=True
USE_SEMANTIC_CHUNKING=True
MAX_WORKERS=4
```

Si tienes una GPU:
```env
USE_GPU=True
N_GPU_LAYERS=32
```

### Problema: "Fall√≥ la instalaci√≥n de dependencias"

**Soluci√≥n:**
```bash
# Actualizar pip y setuptools
pip install --upgrade pip setuptools wheel

# Instalar dependencias una por una
pip install python-dotenv flask pypdf sentence-transformers chromadb llama-cpp-python psutil
```

---

## Pr√≥ximos Pasos

### Aprende M√°s
- üìö Lee [docs/OPTIMIZATIONS.md](docs/OPTIMIZATIONS.md) para ajuste de rendimiento
- üèóÔ∏è Consulta [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) para dise√±o del sistema
- üîå Revisa [docs/API.md](docs/API.md) para detalles de la API

### Personaliza
- Ajusta el tama√±o de fragmentos para mejor contexto
- Prueba diferentes modelos de embeddings
- Configura aceleraci√≥n GPU
- Configura monitoreo

### Explora
- Ejecuta pruebas: `pytest tests/ -v`
- Ver c√≥digo: Navega el directorio `src/`
- Revisa ejemplos: Consulta la carpeta `prototypes/`

---

## Referencia R√°pida

### Comandos Esenciales

```bash
# Interfaz Web
python src/app.py

# CLI - Subir
python src/cli.py upload <ruta-pdf>

# CLI - Preguntar
python src/cli.py ask "tu pregunta"

# CLI - Estad√≠sticas
python src/cli.py stats

# CLI - Listar
python src/cli.py list -v

# Verificar Sistema
python verify_optimizations.py

# Ejecutar Pruebas
pytest tests/ -v
```

### Archivos Importantes

- `.env` - Configuraci√≥n
- `requirements.txt` - Dependencias
- `verify_optimizations.py` - Verificaci√≥n del sistema
- `src/cli.py` - Interfaz de l√≠nea de comandos
- `src/app.py` - Servidor web

---

## Obtener Ayuda

- üìñ **Documentaci√≥n:** Revisa la carpeta `docs/`
- üêõ **Problemas:** Reporta en GitHub
- üí¨ **Preguntas:** Abre una discusi√≥n
- üìß **Contacto:** Consulta los mantenedores del proyecto

---

## Resumen

Ahora deber√≠as tener:
- ‚úÖ ScholarQA instalado y configurado
- ‚úÖ Modelo LLM descargado
- ‚úÖ Sistema verificado y funcionando
- ‚úÖ Primer PDF procesado
- ‚úÖ Preguntas respondidas

**¬°Felicitaciones! ¬°Est√°s listo para usar ScholarQA! üéâ**

---

*Para uso avanzado y consejos de optimizaci√≥n, consulta la documentaci√≥n completa.*
