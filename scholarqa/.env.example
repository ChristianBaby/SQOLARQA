# Configuración de ScholarQA
# Copia este archivo a .env y ajusta los valores según tus necesidades

# === MODELOS ===
# Modelo de embeddings (Sentence Transformers)
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Modelo LLM local (.gguf)
LLM_MODEL=models/downloaded/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf

# === BASE DE DATOS VECTORIAL ===
# Directorio para ChromaDB
CHROMA_PERSIST_DIR=data/vector_store

# Nombre de la colección
COLLECTION_NAME=academic_papers

# Tamaño de batch para ChromaDB
CHROMA_BATCH_SIZE=100

# === SERVIDOR WEB ===
# Host del servidor Flask
FLASK_HOST=0.0.0.0

# Puerto del servidor Flask
FLASK_PORT=5000

# Modo debug (True/False)
FLASK_DEBUG=False

# Tamaño máximo de archivo en MB
MAX_CONTENT_LENGTH=100

# === PROCESAMIENTO DE TEXTO ===
# Tamaño de cada chunk de texto
CHUNK_SIZE=1000

# Overlap entre chunks
CHUNK_OVERLAP=200

# Máximo de chunks por consulta
MAX_CHUNKS_PER_QUERY=5

# Usar chunking semántico (True/False)
USE_SEMANTIC_CHUNKING=True

# === CONFIGURACIÓN DEL LLM ===
# Máximo de tokens a generar
MAX_TOKENS=512

# Temperature (0.0 - 1.0)
TEMPERATURE=0.7

# Top-p sampling
TOP_P=0.95

# Tamaño del contexto
N_CTX=2048

# Capas a cargar en GPU (0 = solo CPU)
N_GPU_LAYERS=0

# === CONFIGURACIÓN DE EMBEDDINGS ===
# Tamaño de batch para embeddings
EMBEDDING_BATCH_SIZE=32

# Normalizar embeddings (True/False)
NORMALIZE_EMBEDDINGS=True

# === CACHÉ ===
# Habilitar caché (True/False)
ENABLE_CACHE=True

# Time-to-live del caché en segundos
CACHE_TTL=3600

# === RENDIMIENTO ===
# Número máximo de workers para procesamiento paralelo
MAX_WORKERS=4

# Usar GPU si está disponible (True/False)
USE_GPU=False

# === LOGGING ===
# Nivel de logging (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Guardar logs en archivo (True/False)
LOG_TO_FILE=True

# Archivo de logs
LOG_FILE=logs/scholarqa.log
